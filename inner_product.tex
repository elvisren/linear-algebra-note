\section{Inner Product}

This section analyzes the length property of vector space. The list of length relations are:
\begin{enumerate}
    \item $\norm{T} = \norm{T^*}$: normal
    \item $\norm{T(x)} = \norm{x}$: unitary (orthogonal)
    \item $\innerproduct{T(x)}{T(y)} = \innerproduct{x}{y}$: unitary (orthogonal)
\end{enumerate}

The relations between $T$ and $T^*$ are:
\begin{enumerate}
    \item $TT^* = T^*T$: normal
    \item $TT^* = T^*T = I$: unitary (orthogonal)
    \item $T^* = \inverse{T}$: unitary (orthogonal)
    \item $T^* = T$: self-adjoint (Hermitian)
\end{enumerate}



\subsection{Definitions}

Inner product is a function $\innerproduct{x}{y}$ that:
\begin{itemize}
    \item $\innerproduct{x + y}{z} = \innerproduct{x}{z} + \innerproduct{y}{z}$    
    \item $\innerproduct{x}{y+z} = \innerproduct{x}{y} + \innerproduct{x}{z}$
    \item $\innerproduct{cx}{y} = c \innerproduct{x}{y}$
    \item $\innerproduct{x}{cy} = \conjugate{c} \innerproduct{x}{y}$
    \item $\conjugate{\innerproduct{x}{y}} = \innerproduct{y}{x}$
    \item $\innerproduct{x}{x} \geq 0$
    \item $\innerproduct{x}{x} = 0$ if and only if $x = 0$
    \item $\innerproduct{x}{0} = \innerproduct{0}{x} = 0$
    \item If $\innerproduct{x}{y} = \innerproduct{x}{z}$ for all $x$, then $y = z$
\end{itemize}

Be careful that inner product could be negative. So it is different from metric which has to be non-negative.

\begin{definition}[conjugate transpose]
    The conjugate transpose (adjoint) of $A_{m \times n}$ is 
    \begin{equation}
        A^* = \conjugate{A^t}
    \end{equation}
\end{definition}

Here are the common inner products:
\begin{itemize}
    \item For $x,y \in F^n$, $\displaystyle \innerproduct{x}{y} = \sum_{i=1}^n x_i \conjugate{y_i}$
    \item For $A,B \in M_{n \times n}$, $\innerproduct{A}{B} = \trace{B^* A}$
    \item For $f,g$, $\displaystyle \innerproduct{f}{g} = \frac{1}{2 \pi} \int_{0}^{2\pi} f(x) \conjugate{g(x)} \dif x$ (inner product space $H$)
\end{itemize}


The norm or length is defined as $\norm{x} = \sqrt{\innerproduct{x}{x}}$

There are several famous equations:
\begin{itemize}
    \item $\norm{cx} = \absolutevalue{c} \norm{x}$
    \item $\norm{x} \geq 0$
    \item $\norm{\innerproduct{x}{y}} \leq \norm{x} \cdot \norm{y}$
    \item $\norm{x+y} \leq \norm{x} + \norm{y}$
\end{itemize}
\begin{proof}
    The proof relies on $\norm{x} \geq 0$.
\end{proof}


If $x \perp y$, we have $\norm{x+y}^2 = \norm{x}^2 + \norm{y}^2$


\begin{definition}
    For a inner product space $V$:
    \begin{enumerate}
        \item unit vector: $\norm{x} =1$
        \item orthogonal: $\innerproduct{x}{y} = 0$
        \item orthonormal: a subset $S$ of $V$ is orthonormal if $S$ consists of orthogonal unit vectors
    \end{enumerate}
\end{definition}

A orthogonal set is linearly independent.

The orthonormal basis of inner product space $H$ are $f_n(t) = e^{int}$ for $n \in R^+$ because $\innerproduct{f_m}{f_n} = \delta_{mn} $.

If the basis matrix $Q$ is orthonormal, then $\inverse{Q} = Q^*$, which is called unitary or orthogonal in \defiref{unitary}.



% orthogonal
\subsection{Orthogonal Projection}

For an orthogonal subset $S=\set{v_1, v_2, ..., v_k}$ of $V$, the projection $u$ of $y$ on $S$ is 
\begin{equation}
    u = \sum_{i=1}^k \frac{\innerproduct{y}{v_i}}{\innerproduct{v_i}{v_i}} v_i
\end{equation}

If $y \in \spanset{S}$, then $\displaystyle y = u$. If $S$ is orthonormal, then $\displaystyle y = \sum_{i=1}^k \innerproduct{y}{v_i} v_i$. 

The orthogonal projection $u$ is the closest vector to $y$. So if there is any other $x \in \spanset{S}$, $\norm{y - x} \geq \norm{y - u}$. Another property is $y - u \perp \spanset{S}$.


\begin{definition}[orthogonal complement]
    For a subset $S$ (not need to be vector space) of $V$, the orthogonal complement $S^\perp$ is defined as $S^\perp=\set{x \in V: \forall y \in S, \innerproduct{x}{y} = 0}$.
\end{definition}



\begin{theorem}[Gram-Schmidt process]\label{gram_schmidt_process}
    The steps of converting a linearly independent subset $S=\set{w_1, w_2, ..., w_k}$ into orthogonal set $S^\prime=\set{v_1, v_2, ..., v_k}$. The idea is to 
    \begin{enumerate}
        \item Project $v_{j}$ onto $\set{v_1, v_2, ..., v_{j-1}}$
        \item Remove the projection from $v_j$
        \item (Optional) normalize $S^\prime$ by dividing $v_i$ with its norm: $\displaystyle \frac{v_i}{\norm{v_i}}$
    \end{enumerate}
    
    The first two steps can be summarized as:
    \begin{equation}
        v_j = w_j - \sum_{i=1}^{j-1} \frac{\innerproduct{w_j}{v_i}}{\innerproduct{v_i}{v_i}} v_i
    \end{equation}
\end{theorem}

Once we have orthonormal basis $\beta = \set{v_1, v_2, ..., v_n}$,  we could directly represent vectors and linear operators using orthonormal basis:
\begin{itemize}
    \item $\displaystyle x = \sum_{i=1}^n \innerproduct{x}{v_i} v_i$
    \item $\left(\coordinate{T}_\beta\right)_{ij} = \innerproduct{T(v_j)}{v_i}$ (extremely useful. The order of $i$ and $j$ are swapped)
    \item For $g: V \rightarrow F$, $g(x) = \innerproduct{x}{\sum_{i=1}^n \conjugate{g(v_i)} v_i}$
\end{itemize}



\theoref{split_means_upper_trangular} did not give requirement on the basis $\beta$. In fact we could ask $\beta$ to be orthonormal according to the theorem below.

\begin{theorem}[Schur]\label{schur_theorem}
    If the characteristic polynomial of $T$ splits, then there exists an orthonormal basis $\gamma$ such that $\coordinate{T}_\gamma$ is upper triangular.
\end{theorem}
\begin{proof}
    If $T$ splits, there is a basis $\beta$ that $\coordinate{T}_\beta$ is upper triangular. Then use the Gram-Schmidt process (\theoref{gram_schmidt_process}) to normalize $\beta$ and we have $\gamma$.
\end{proof}



% adjoint
\subsection{Adjoint}


\begin{theorem}
    Let $T$ be linear operator on a finite-dimension inner product space $V$. There is a unique linear operator $T^*$ that $\innerproduct{T(x)}{y} = \innerproduct{x}{T^* (y)}$.
\end{theorem}
\begin{proof}
    Linearity is easy to prove. To find $T^*$, for each $y$, there is a function $g_y (x) = \innerproduct{T(x)}{y} = \innerproduct{x}{g^\prime (y)}$. This $g^\prime$ is the $T^*$.
\end{proof}

$T^*$ has a nice property that $\innerproduct{x}{T(y)} = \innerproduct{T^* (x)}{y}$. So we could move $T$ left or right inside the inner product.

\begin{theorem}\label{operator_conjugate_transpose_requirement}
    Let $\beta$ be an orthonormal basis of $V$, for a linear operator $T$, we have
    \begin{equation}
        \coordinate{T^*}_\beta = \left(\coordinate{T}_\beta\right)^*
    \end{equation}
    Be very careful that \emph{$\beta$ must be orthonormal.}
\end{theorem}
\begin{proof}
    Because $\beta$ is orthonormal, we have
    \begin{equation*}
        \left(\coordinate{T^*}_\beta\right)_{ij} = \innerproduct{T^* (v_j)}{v_i} = \innerproduct{v_j}{T(v_i)} = \conjugate{\innerproduct{T(v_i)}{v_j)}} = \conjugate{(\coordinate{T}_\beta))_{ji}}
    \end{equation*}
\end{proof}

\begin{theorem}
    Let $L_A$ be a linear operator. We have
    \begin{equation}
        \rangespace{L_{A^*}}^\perp = \nullspace{L_A}
    \end{equation}
    It means $\rangespace{L_{A^*}}$ and $\nullspace{L_A}$ are orthogonal to each other and they are the direct sum of $V$. This formula is \emph{very useful} because it find two perpendicular subspaces from $A$.
\end{theorem}
\begin{proof}
    For $x \in \nullspace{L_A}$ and $y \in \rangespace{L_{A^*}}$, we have $\innerproduct{A^*y}{x} = \innerproduct{y}{Ax} = \innerproduct{y}{0} = 0$. So $x \perp A^*y$.
\end{proof}



\begin{theorem}\label{congjugate_transpose_has_eigenvalue}
    If $T$ has eigenvalue, then $T^*$ has eigenvalue.    
\end{theorem}
\begin{proof}
    \begin{equation*}
        0 = \innerproduct{(T-\lambda I)v}{x} = \innerproduct{v}{(T-\lambda I)^* x} = \innerproduct{v}{(T^* - \conjugate{\lambda}I) x}
    \end{equation*}
    
    So $v \perp \spanset{T^* - \conjugate{\lambda}I}$, which means $T^* - \conjugate{\lambda}I$ has nonzero null space.
\end{proof}


\begin{theorem}
    Let $T$ be linear operator, we have that $\norm{T(x)} = \norm{x}$ if and only if $\innerproduct{T(x)}{T(y)} = \innerproduct{x}{y}$.
    
    Note: it will be proved later that $T$ is unitary (or orthogonal).
\end{theorem}
\begin{proof}
    Use the property that $\innerproduct{x}{y} = \frac{1}{4}\norm{x+y}^2 - \frac{1}{4}\norm{x-y}^2$.
\end{proof}



\begin{example}[Least Square]
    For a list of points $(x_i, y_i)$, we want to find $x_0$ that $\norm{y - Ax_0}$ is minimum. $Ax$ is a subspace, so we want to project $y$ on it and find the projection. If $Ax_0$ is the projection, then $y - Ax_0 \perp Ax$, so $\innerproduct{Ax}{y - Ax_0} = 0$. so $\innerproduct{x}{A^* (Ax_0 - y)} = 0$ for all $x$, which means $A^* (Ax_0 - y) = 0$.
\end{example}

\begin{example}[Minimal Solution]
    For all solutions to $Ax=b$, we want to find the solution with minimal norm $\norm{s}$. Define $W = \rangespace{L_{A^*}}$ and $W^\perp = \nullspace{L_A}$. Because $W \perp W^\perp$, we have $x = s + y$ where $s \in W$ and $y \in W^\perp$. So $b = Ax = As + Ay = As$ and $s$ is a solution.
    
    To prove $s$ is minimal, use the property that if $s \perp y$, then $\norm{s+y}^2 = \norm{s}^2 + \norm{y}^2$. So $s$ is the only solution to $Ax=b$ that lies in $\rangespace{L_{A^*}}$.
    
    What's more, if $(AA^*)u = b$, then $s = A^* u$. This is because $s = A^* u \in \rangespace{L_{A^*}}$ and $As = AA^*u = b$. $s$ is the only vector with such property.
    
    Importance of this example: in linear equation chapter we have proved that the solution to $Ax=b$ are $x_0 +  \nullspace{L_A}$. Here $x_0$ could be any solution to $Ax=b$. This example shows we could find a very special solution $x_0 = s$ that $s \perp \nullspace{L_A}$.
\end{example}




% normal
\subsection{Normal and Self-adjoint Operators}

The purpose here is to find a special basis of $V$:
\begin{itemize}
    \item It has to be orthonormal
    \item It is eigenvector of $T$
\end{itemize}


This obviously puts requirement on both the $V$ and $T$. It turns out that the $T$ must be normal (in $C$) or self-adjoint (in $R$). So we prefer complex normal and real symmetric matrices. The operators here are complex normal or real symmetric.

\begin{definition}[Normal]
    $T$ is normal if $TT^* = T^* T$. $A$ is normal if $AA^* = A^* A$.
\end{definition}

A common mistake is to think that $T$ is normal if and only if $\coordinate{T}_\beta$ is normal. $\beta$ must be orthonormal according to \theoref{operator_conjugate_transpose_requirement}.

$T$ will have many nice properties if it is normal:
\begin{itemize}
    \item $\norm{T(x)} = \norm{T^* (x)}$
    \item $T - cI$ is normal
    \item If $(T - \lambda I)x = 0$, then $(T^* - \conjugate{\lambda}I)x = 0$. So $T$ and $T^*$ share the same eigenvector but the eigenvalues are conjugate
    \item If $a$ and $b$ are eigenvector for eigenvalue $\lambda_1$ and $\lambda_2$, then $a \perp b$
\end{itemize}
\begin{proof}
    \begin{equation*}
        \norm{T(x)}^2 = \innerproduct{T(x)}{T(x)} = \innerproduct{T^*T(x)}{x} = \innerproduct{TT^*(x)}{x} = \innerproduct{T^*(x)}{T^*(x)} = \norm{T^*(x)}^2
    \end{equation*}
    \begin{equation*}
        (T-cI)(T-cI)^* = (T-cI)(T^* - \conjugate{c}I) = ... = (T-cI)^*(T-cI)
    \end{equation*}
    Because $T-cI$ is normal when $T$ is normal, so
    \begin{equation*}
        0 = \norm{(T-cI)x} = \norm{(T-cI)^* x} =  \norm{(T^*- \conjugate{\lambda} I)x}
    \end{equation*}
    which means $(T^*- \conjugate{\lambda} I)x = 0$. So $\conjugate{\lambda}$ is an eigenvalue of $T^*$ and $x$ is its eigenvector.
    \begin{equation*}
        \lambda_1 \innerproduct{a}{b} = \innerproduct{\lambda_1 a}{b} = \innerproduct{T(a)}{b} = \innerproduct{a}{T^*(b)} = \innerproduct{a}{\conjugate{\lambda_2}b} = \lambda_2 \innerproduct{a}{b} \Rightarrow (\lambda_1 - \lambda_2)\innerproduct{a}{b} = 0
    \end{equation*}
    Since $\lambda_1 \neq \lambda_2$, we have $\innerproduct{a}{b} = 0$ and $a \perp b$.
\end{proof}



Now we come the most important theorem of this section.

\begin{theorem}\label{normal_orthonormal_with_complex_space}
    For finite-dimension inner product space $V$ on $F=C$, $T$ is normal if and only if there exists orthonormal basis that are eigenvectors of $T$.
\end{theorem}
\begin{proof}
    If such basis exists, the coordinates of $T$ and $T^*$ are diagonal matrix, so they commute. We do not need the complex feature here. The difficult is on the other side of proof and we need the complex property.
    
    Because complex polynomials always split, by \theoref{schur_theorem}, there is orthonormal $\beta$ that $A=\coordinate{T}_\beta$ is an upper triangular matrix. Assume the first $k-1$ columns are eigenvectors, let's see what happens to $v_{k}$. Since $A$ is upper triangular, we have
    \begin{equation*}
        T(v_k) = A_{1k} v_1 + A_{2k} v_2 + ... + A_{kk} v_k
    \end{equation*}
    Because $\beta$ is orthonormal, we could directly calculate $A_{jk}$:
    \begin{equation*}
        A_{jk} = \innerproduct{T(v_k)}{v_j} = \innerproduct{v_k}{T^*(v_j)} = \innerproduct{v_k}{\conjugate{\lambda_j} v_j} = \lambda \innerproduct{v_k}{v_j} = 0
    \end{equation*}
    
    Normality is use here. Although $T^*$ always have eigenvalue if $T$ has eigenvalue, the eigenvalue $v_j$ of $T$ may not be the eigenvalue of $T^*$.
    
    So the result is a diagonal matrix and $\beta$ are all eigenvectors.
\end{proof}

One corollary is that a normal transformation can be diagonalized because we could find a basis  of eigenvectors.

\theoref{normal_orthonormal_with_complex_space} did not work for $F=R$ because polynomials do not always split. So we need stronger restriction to make it split in $R$, which is self-adjoint.

\begin{definition}[Hermitian, self-adjoint]
    $T$ is Hermitian (self-adjoint) if $T = T^*$. So is it for $A$.
\end{definition}

If $T$ is self-adjoint, it has even better properties:
\begin{enumerate}
    \item Every eigenvalue of $T$ is real, regardless of $F$ is $C$ or $R$
    \item If $V$ is on $F=R$, $T$ still splits
\end{enumerate}
\begin{proof}
    \begin{equation*}
        \lambda x = Tx = T^* x = \conjugate{\lambda} x
    \end{equation*}
    So $\lambda = \conjugate{\lambda}$ and $\lambda$ is real.
    
    For second property, take $F=C$ and let $T$ splits in $C$. But all eigenvalues are real, so $T$ splits in $R$.
\end{proof}

\begin{theorem}
    For finite-dimension inner product space $V$ on $F=R$, $T$ is self-adjoint if and only if there exists orthonormal basis that are eigenvectors of $T$.
\end{theorem}
\begin{proof}
    If $T$ is self-adjoint, it splits. So we apply \theoref{schur_theorem} to find basis $\beta$ that $A = \coordinate{T}_\beta$ is upper triangular. But
    \begin{equation*}
        A^* = \left(\coordinate{T}_\beta\right)^* = \coordinate{T^*}_\beta = \coordinate{T}_\beta = A
    \end{equation*}
    
    So $A$ is a diagonal matrix and $\beta$ are eigenvectors.
\end{proof}

A quick summary of important theorems:
\begin{enumerate}
    \item If $T$ splits, there is $\beta$ that $\coordinate{T}_\beta$ is upper triangular
    \item $\beta$ could be orthonormal using Gram-Schmidt process
    \item $\beta$ could be orthonormal and eigenvectors if $T$ is normal (in $C$)
    \item $\beta$ could be orthonormal and eigenvectors if $T$ is self-adjoint (in $R$)
    \item normal means diagonalizable
\end{enumerate}


So the conclusion is, if the matrix $A$ is complex normal or real symmetric, there is orthonormal basis of eigenvectors of $A$.

% unitary and orthogonal operators
\subsection{Unitary and Orthogonal Operators}

Adjoint operation ($T^*$ and $T$) works pretty like conjugate in $C$. There is a special $i \in C$  that $i \conjugate{i} = 1$, so we would like to examine what happens when $TT^* = T^* T = I_n$. It turns out that these $T$ could preserve length.

\begin{definition}\label{unitary}
    If $\norm{T(x)} = \norm{x}$, $T$ is called unitary if $F=C$ and orthogonal if $F=R$.
\end{definition}




\begin{theorem}\label{normal_zero_operator}
    If $T$ is normal and $\innerproduct{x}{Tx} = 0$ for all $x$, then $T=0$.
\end{theorem}
\begin{proof}
    The trick is in using $TT$:
    \begin{equation*}
        0 = \innerproduct{x + T(x)}{T(x + T(x))} = 2 \innerproduct{T(x)}{T(x)}
    \end{equation*}
    So $\innerproduct{Tx}{Tx} = 0$ for all $x$, and $T = 0$
\end{proof}


If $T$ is unitary, it has many good properties. All the followings are equivalent:
\begin{enumerate}
    \item $T^*T = I$
    \item $TT^* = I$
    \item $\innerproduct{T(x)}{T(y)} = \innerproduct{x}{y}$
    \item If $\beta$ is an orthonormal basis, then $T(\beta)$ is also an orthonormal basis
    \item There exists an orthonormal basis $\beta$ that $T(\beta)$ is also an orthonormal basis
    \item $\norm{T(x)} = \norm{x}$
\end{enumerate}
\begin{proof}
    Assume $TT^* = I$
    \begin{equation*}
        \innerproduct{T(x)}{T(y)} = \innerproduct{x}{T^*Ty} = \innerproduct{x}{y}
    \end{equation*}
    
    If $\beta = \set{v_i}$ are orthonormal basis, we have $\innerproduct{T(v_i)}{T(v_j)} = \innerproduct{v_i}{v_j} = \delta_{ij}$, so $T(\beta)$ is also an orthonormal basis.
    
    If $\norm{T(x)} = \norm{x}$, we have 
    \begin{equation*}
        \innerproduct{x}{x} = \innerproduct{T(x)}{T(x)} = \innerproduct{x}{T^*Tx}
    \end{equation*}
    which means $\innerproduct{x}{(T^*T - I)x} = 0$. Because $T^*T - I$ is normal, Theorem~\ref{normal_zero_operator} shows $T^*T = I$
\end{proof}

So unitary (or orthogonal) means $\inverse{T} = T^*$.

These properties shows that if $T$ is unitary, it preserves inner product ($\innerproduct{T(x)}{T(y)} = \innerproduct{x}{y}$) and length ($\norm{T(x)} = \norm{x}$).


If $AA^* = I$, the rows of $A$ are orthonormal basis for $F^n$. Also if $A^*A = I$, then the columns are orthonormal basis.


\begin{theorem}
    If $V$ is on $R$, it has an orthonormal basis of eigenvectors with $\absolutevalue{\lambda_i} = 1$ if and only if $T$ is both self-adjoint and orthogonal.     
    
    If $V$ is on $C$, $T$ needs to be unitary.
    
    In another term, if $T$ is in $C$, then $T^* = \inverse{T}$. But if $T$ is in $R$, then $T = T^* = \inverse{T}$.
\end{theorem}
\begin{proof}
    If $V$ has orthonormal basis with eigenvectors of $T$, $T$ is self-adjoint (or normal). So $T(T^* v_i) = T (\lambda_i v_i) = \lambda_i (T v_i) = \lambda_i^2 v_i$. Because $\absolutevalue{\lambda_i} = 1$, we have $TT^* v_i = v_i$, so $TT^* = I$ and $T$ is orthogonal.
    
    Then for the $R$ case. If $T$ is self-adjoint, there exists orthonormal basis of eigenvectors of $T$. Now if $T$ is also orthogonal, we have $\absolutevalue{\lambda_i} \norm{v_i} = \norm{\lambda_i v_i} = \norm{T (v_i)} = \norm{v_i}$, so $\absolutevalue{\lambda_i} = 1$. 
    
    For the $C$ case, unitary means normal and we have the same proof.
\end{proof}



If the matrix $A$ is complex normal or real symmetric, there is orthonormal basis of eigenvectors of $A$. These basis form a matrix $Q$ and $D = \inverse{Q} A Q$. This $Q$ is orthogonal because its columns are orthonormal basis. So $\inverse{Q} = Q^*$. Here $D$ and $A$ are called unitarily equivalent (or orthogonally equivalent). Here we did not require $A$ to be unitary or orthogonal.

With the definition of unitarily equivalent, the necessary and sufficient condition of being normal could be rephrased:
\begin{theorem}
    Let $A$ in $C$. $A$ is normal if and only if $A$ is unitarily equivalent to a diagonal matrix. If $A$ is in $R$, then the condition is orthogonally equivalent.
\end{theorem}




% rigid motion
\subsection{Rigid Motion}

Rigid motion analyze how to move a thing without changing its shape.

\begin{definition}[Rigid Motion]
    Let $V$ be real inner product space. A function $f$ is called rigid motion if 
    \begin{equation}
        \norm{f(x) - f(y)} = \norm{x - y}
    \end{equation}
\end{definition}

One rigid motion example is rotation:
\begin{equation}
    \begin{pmatrix}
        \cos \theta & - \sin \theta \\
        \sin \theta & \cos \theta
    \end{pmatrix}
\end{equation}


We have two more example of rigid motion: reflection and translation.

\begin{definition}[Reflection]
    Let $L$ be a linear in $R^2$ that passes through the original. $T$ is called reflection about $L$ if $T(x) = x$ for $x \in L$ and $T(x) = -x$ for $x \in L^\perp$.
\end{definition}

If $L$ has angle $\theta$, reflection $T$ has matrix representation 
\begin{equation}
    \begin{pmatrix}
        \cos 2\theta & \sin 2\theta \\
        \sin 2\theta & -\cos 2\theta
    \end{pmatrix}
\end{equation}

\begin{definition}[Translation]
    For a vector $v$, a translation $T$ is $T(x) = x + v$ for all $x$.
\end{definition}

If $f$ is an orthogonal operator, then $\norm{f(x-y)} = \norm{x-y}$, so $f$ is a rigid motion. A orthogonal operator followed by translation is also a rigid motion. The following theorem proves this is the only case.

\begin{theorem}
    Let $f$ be a rigid motion. Then there exists a unique orthogonal operator $T$ and a unique translation $g$ that $f = g  \circ T$.
\end{theorem}
\begin{proof}
    Define $T(x) = f(x) - f(0)$ and $g(x) = x + f(0)$. Then $\norm{T(x)} = \norm{f(x) - f(0)} = \norm{x - 0} = \norm{x}$. Replace $x$ by $x-y$, we have $\innerproduct{T(x)}{T(y)} = \innerproduct{x}{y}$.
    
    The next step is to prove $T$ is a linear transformation. Prove it by proving 
    \begin{equation*}
        \norm{T(x + a y) - T(x) - aT(y)}^2 = 0
    \end{equation*}
    
    If $T$ is linear and preserves inner product, it is orthogonal.
\end{proof}


It turns out there are only two orthogonal operators in $R^2$. Assume $\beta$ is the standard basis of $R^2$ and let $A = \coordinate{T}_\beta$:
\begin{enumerate}
    \item rotation, with $\determinate{A} = 1$
    \item reflection, with $\determinate{A} = -1$
\end{enumerate}
\begin{proof}
    Since $T$ is orthogonal, $T(\beta)$ is also orthonormal. Assume $T(e_1) = (\cos \theta, \sin \theta)$, the only choice for the other vector is $T(e_2) = (- \sin \theta, \cos \theta)$ or $T(e_2) = (\sin \theta, - \cos \theta)$.
    
    If $T(e_2) = (- \sin \theta, \cos \theta)$, $A = \begin{pmatrix}
                \cos \theta & - \sin \theta \\
        \sin \theta & \cos \theta
    \end{pmatrix}$, so it is a rotation by $\theta$.
    
    If $T(e_2) = (\sin \theta, - \cos \theta)$, $A = \begin{pmatrix}
        \cos \theta & \sin \theta \\
        \sin \theta & -\cos \theta
    \end{pmatrix}$, so it is a reflection along a line $L$ with $\displaystyle \frac{\theta}{2}$.
\end{proof}

So in $R^2$, a rigid motion is either a rotation or reflection followed by a translation.

\begin{example}[quadratic form]
    Now consider the quadratic form $ax^2 + 2bxy + cy^2$. If we let 
\begin{equation}
    A = \begin{pmatrix}
        a & b \\
        b & c
    \end{pmatrix} \text{ and } X = \begin{pmatrix}
        x \\
        y
    \end{pmatrix}
\end{equation}

The quadratic form becomes $X^t A X = \innerproduct{AX}{X}$. Because $A$ is symmetric, there exists an orthogonal matrix $P$ that $D = P^t A P$. Now let $X^\prime = P^t X$, we have $PX^\prime = PP^tX = X$. So $X^t A X = (PX^\prime)^t A (PX^\prime) = {X^\prime}^t (P^t A P) X^\prime = {X^\prime}^t D X^\prime = \lambda_1 (x_1^\prime)^2 + \lambda_2 (x_2^\prime)^2$. Now the quadratic form is simplified.

$P$ may not be the final result because it is possible that $\determinate{P} = -1$. In this case, we could exchange the column of $P$. Now $\lambda_1$ and $\lambda_2$ need to switch order.

The end result is that a quadratic form could be simplified by a rotation.
\end{example}
















































