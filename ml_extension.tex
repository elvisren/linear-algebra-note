\section{Machine Learning Extension}


The standard basis $\set{e_i}$ for $\realset^n$ is called one-hot vector.

$A_{i,:}$ select the $i$th column, and $A_{:,j}$ select the $j$th row.

A tensor is a $n$-dimension array. The $n$ is called order or rank.


\begin{definition}[Hadamard Product]
    The Hadamard product of $A_{m \times n}$ and $B_{m\times n}$ is defined as $(A \odot B)_{ij} = A_{ij} \cdot B_{ij}$.
\end{definition}

\subsection{Norm}

\begin{definition}[Norm]
    The norm is a function $f : R^n \rightarrow R$ that:
    \begin{itemize}
        \item $f(x) \geq 0$
        \item $f(x) = 0$ if and only if $x = 0$
        \item $f(tx) = t f(x)$
        \item $f(x+y) \leq f(x) + f(y)$
    \end{itemize}
\end{definition}

\begin{definition}
    The norms of a vector $x$ are:
\begin{itemize}
    \item p-norm: $\displaystyle \norm{x}_p = (\sum_{i=1}^n \absolutevalue{x_i}^p)^{\frac{1}{p}}$
    \item 2-norm: $\displaystyle \norm{x}_{p=2} = \transpose{x} x$
    \item 1-norm: $\displaystyle \norm{x}_1 = \sum_{i=1}^n \absolutevalue{x_i}$
    \item 0-norm: $\displaystyle \norm{x}_0 = \sum_{i=1}^n x^0$, if we define $0^0 = 0$.
    \item max-norm: $\norm{x}_\infty = \max \set{\absolutevalue{x_i}}$
\end{itemize}
\end{definition}

\begin{definition}[Induced Norm]
   For a matrix $A_{m \times n}$, its induced norm is
\begin{equation}
    \norm{A}_p = \max\limits_{\norm{x}=1} \norm{Ax}_p
\end{equation}

If $p=2$, $\norm{A}_2 = \sqrt{\lambda_{\mathrm{max}} ( \transpose{A}A}) = \max\limits_i \sigma_i$, where $\sigma_i$ is the $i$th singnular value.
 
\end{definition}

\begin{definition}[Trace Norm]
    The trace norm, also called nuclear norm, is defined as
\begin{equation}
    \norm{A}_* = \trace{\sqrt{\transpose{A}A}} = \sum_i \sigma_i = \norm{\sigma}_1
\end{equation}
\end{definition}

\begin{definition}[Forbenius Norm]
    \begin{equation}
        \norm{A}_F = \sqrt{\sum_i \sum_j A_{ij}^2} = \sqrt{\trace{\transpose{A}A}}
    \end{equation}
\end{definition}


\subsection{Properties}

\begin{theorem}[Cyclic Permutation Property]
    For $A$,$B$,$C$, if the result of $ABC$ is a square matrix, then
    \begin{equation}
        \trace{ABC} = \trace{BCA} = \trace{CAB}
    \end{equation}
    
    One of the result is that for vector $x$, we have
    \begin{equation}
        \transpose{x} A x = \trace{\transpose{x}Ax} = \trace{x \transpose{x} A}
    \end{equation}
\end{theorem}

\begin{theorem}
    For $A_{m \times n}$, we have
    \begin{equation}
        \rank{A} = \rank{\transpose{A}} = \rank{\transpose{A}A} = \rank{A \transpose{A}}
    \end{equation}    
\end{theorem}


\begin{theorem}
    The cosine of the angle between $x$ and $y$ is
    \begin{equation}
        \cos(x,y) = \frac{\transpose{x}y}{\norm{x} \norm{y}}
    \end{equation}
    
    If we apply orthogonal transformation on $x$ and $y$, the angle will not change.
\end{theorem}
\begin{proof}
    For orthogonal transformation $T$, $\displaystyle \cos(Tx, Ty) = \frac{\transpose{(Tx)} Ty}{\norm{Tx} \norm{Ty}} = \cos(x,y)$. So orthogonal transformation preserve both the angle and norm, and it is either rotation or reflection.
\end{proof}


\subsection{Matrix Calculus}

\begin{definition}[Gradient]
    For $f: \realset^n \rightarrow \realset$, the derivative against $x$ is called gradient:
    \begin{equation}
    G_f (x) = \nabla f_{n \rightarrow 1} (x) = \begin{pmatrix}
        \pd{f}{x_1} \\
        \vdots \\
        \pd{f}{x_n}
    \end{pmatrix}
    \end{equation}
    
    To be consistent with Jacobian matrix, it is preferred that
    \begin{equation}
        \nabla f (x) = \begin{pmatrix}
            \pd{f}{x_1}, \hdots, \pd{f}{x_n}
        \end{pmatrix}
    \end{equation}
    
    So $\nabla f: \realset^n \rightarrow \realset^n$ is a vector field.
\end{definition}



\begin{example}    
For directional derivative of $f$ along a vector $v$, we have 
\begin{equation}
    D_v f(x) = \lim\limits_{h \rightarrow 0} \frac{f(x + hv) - f(x)}{h}
\end{equation}

We have $D_v f(x) = \nabla f(x) \cdot v$.
\end{example}

\begin{definition}[Jacobian Matrix]
    For $f: \realset^n \rightarrow \realset^m$, the derivative against $x$ is called Jacobian matrix:
    \begin{equation}
        J_{f_{n \rightarrow m}} (x) = \begin{pmatrix}
            \nabla f_1 \\
            \nabla f_2 \\
            \vdots \\
            \nabla f_m
        \end{pmatrix}_{m \times n}
    \end{equation}
    
%    So $J_f$ is a matrix that transform from $\realset^n$ to $\realset^m$, so it is an $m \times n$ matrix.
\end{definition}


\begin{definition}[Hessian Matrix]
    For $f: \realset^n \rightarrow \realset$, the second partial derivatives is
    \begin{equation}
        H_f = \nabla^2 f = \begin{pmatrix}
            \pd[2]{f}{x_1} & \hdots & \md{f}{2}{x_1}{1}{x_n}{1} \\
            & \vdots \\
            \md{f}{2}{x_n}{1}{x_1}{1} & \hdots & \pd[2]{f}{x_n}
        \end{pmatrix}
    \end{equation}
    
    So Hessian matrix is the Jacobian of the gradient.
\end{definition}





\begin{definition}[Chain Rule]
    Let $h(x) = g \circ f (x)$, we have
    \begin{equation}
        J_h = J_g \times J_f
    \end{equation}
\end{definition}



\begin{example}
    Useful result for $f:\realset^n \rightarrow \realset$:
    \begin{equation}
        \pd{\transpose{a}x}{x} = a
    \end{equation}   
    \begin{equation}
        \pd{\transpose{b}Ax}{x} = \transpose{A}b
    \end{equation}
    \begin{equation}
        \pd{\transpose{x}Ax}{x} = (A + \transpose{A}) x
    \end{equation}
\end{example}

\begin{definition}
    If $x$ is not a vector but a matrix and $f: \realset^{m \times n} \rightarrow \realset$, we define
    \begin{equation}
        \pd{f}{X} = \begin{pmatrix}
            \pd{f}{x_{1,1}} & \hdots & \pd{f}{x_{1,n}} \\
            & \vdots \\
            \pd{f}{x_{m,1}} & \hdots & \pd{f}{x_{m,n}}
        \end{pmatrix}
    \end{equation}
\end{definition}

\begin{example}
    Useful result for    $f: \realset^{m \times n} \rightarrow \realset$:
    \begin{equation}
        \pd{\transpose{a}X b}{X} = a \transpose{b}
    \end{equation} 
    \begin{equation}
        \pd{\transpose{a} \transpose{X} b}{X} = b \transpose{a}
    \end{equation}
\end{example}






A short summary:
\begin{itemize}
    \item For $f:\realset^n \rightarrow \realset$, we have $\nabla f$, $H_f$
    \item For $f:\realset^n \rightarrow \realset^m$, we only have $\nabla f$. We do not have matrix representation of $H_f$
    \item For $f: \realset^{m \times n} \rightarrow \realset$, we have another form of partial derivatives
    \item The derivative is the transpose:
        \begin{itemize}
            \item $\transpose{a}x \rightarrow a$
            \item $\transpose{b}A x \rightarrow \transpose{A}b$
            \item $\transpose{x}A x \rightarrow (A + \transpose{A}) x$
            \item $\transpose{a} X b \rightarrow a \transpose{b}$
            \item $\transpose{a} \transpose{X} b \rightarrow b \transpose{a}$    
        \end{itemize}

\end{itemize}































